{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARN_DATA_SIZE = 1000\n",
    "INPUT_SIZE = 784\n",
    "OUTPUT_SIZE = 10\n",
    "BATCH_SIZE = 100\n",
    "HIDDEN_SIZE = 100\n",
    "ALPHA = 2\n",
    "TOTAL_LAYERS_AMOUNT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return (x > 0) * x\n",
    "\n",
    "def relu_deriv(x):\n",
    "    return x > 0\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_deriv(x):\n",
    "    return 1 - (x ** 2)\n",
    "\n",
    "def softmax(x):\n",
    "    tmp = np.exp(x)\n",
    "    return tmp / np.sum(tmp, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "(images_train, labels_train), (images_test, labels_test) = mnist.load_data()\n",
    "\n",
    "learn_data, learn_outputs = images_train[:LEARN_DATA_SIZE] / 255, labels_train[:LEARN_DATA_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 28, 28) (1000,)\n",
      "(1000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(learn_data.shape, learn_outputs.shape)\n",
    "learn_data = learn_data.reshape(LEARN_DATA_SIZE, INPUT_SIZE)\n",
    "print(learn_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10)\n"
     ]
    }
   ],
   "source": [
    "tmp = np.zeros(shape=(LEARN_DATA_SIZE, OUTPUT_SIZE), dtype=np.float32)\n",
    "for ind, out in enumerate(learn_outputs):\n",
    "    tmp[ind][out] = 1.0\n",
    "learn_outputs = tmp\n",
    "print(learn_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "test_data = images_test / 255\n",
    "test_outputs = np.zeros(shape=(labels_test.shape[0], 10), dtype=np.float32)\n",
    "for ind, out in enumerate(labels_test):\n",
    "    test_outputs[ind][out] = 1.0\n",
    "test_ouputs = softmax(test_outputs)\n",
    "print(test_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "weights = [\n",
    "    0.02 * np.random.rand(INPUT_SIZE, HIDDEN_SIZE) - 0.01,\n",
    "    0.2 * np.random.rand(HIDDEN_SIZE, OUTPUT_SIZE) - 0.1\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 10 Learn-Acc: 0.94 Test-Acc: 0.85\n",
      "Iter: 20 Learn-Acc: 0.98 Test-Acc: 0.86\n",
      "Iter: 30 Learn-Acc: 0.99 Test-Acc: 0.87\n",
      "Iter: 40 Learn-Acc: 1.00 Test-Acc: 0.88\n",
      "Iter: 50 Learn-Acc: 0.99 Test-Acc: 0.88\n",
      "Iter: 60 Learn-Acc: 0.99 Test-Acc: 0.87\n",
      "Iter: 70 Learn-Acc: 1.00 Test-Acc: 0.88\n",
      "Iter: 80 Learn-Acc: 1.00 Test-Acc: 0.88\n",
      "Iter: 90 Learn-Acc: 1.00 Test-Acc: 0.88\n",
      "Iter: 100 Learn-Acc: 1.00 Test-Acc: 0.87\n",
      "Iter: 110 Learn-Acc: 1.00 Test-Acc: 0.88\n",
      "Iter: 120 Learn-Acc: 1.00 Test-Acc: 0.88\n",
      "Iter: 130 Learn-Acc: 1.00 Test-Acc: 0.88\n",
      "Iter: 140 Learn-Acc: 1.00 Test-Acc: 0.87\n",
      "Iter: 150 Learn-Acc: 1.00 Test-Acc: 0.88\n",
      "Iter: 160 Learn-Acc: 1.00 Test-Acc: 0.87\n",
      "Iter: 170 Learn-Acc: 1.00 Test-Acc: 0.88\n",
      "Iter: 180 Learn-Acc: 1.00 Test-Acc: 0.88\n",
      "Iter: 190 Learn-Acc: 1.00 Test-Acc: 0.88\n",
      "Iter: 200 Learn-Acc: 1.00 Test-Acc: 0.88\n",
      "Iter: 210 Learn-Acc: 1.00 Test-Acc: 0.87\n",
      "Iter: 220 Learn-Acc: 1.00 Test-Acc: 0.88\n",
      "Iter: 230 Learn-Acc: 1.00 Test-Acc: 0.88\n",
      "Iter: 240 Learn-Acc: 1.00 Test-Acc: 0.88\n",
      "Iter: 250 Learn-Acc: 1.00 Test-Acc: 0.88\n",
      "Iter: 260 Learn-Acc: 1.00 Test-Acc: 0.87\n",
      "Iter: 270 Learn-Acc: 1.00 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18228\\1152702382.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mlayers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mTOTAL_LAYERS_AMOUNT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mINPUT_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m             \u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1, 301):\n",
    "    correct_counter = 0\n",
    "    for j in range(LEARN_DATA_SIZE // BATCH_SIZE):\n",
    "        batch_start, batch_end = j * BATCH_SIZE, (j + 1) * BATCH_SIZE\n",
    "\n",
    "        layers = [None] * TOTAL_LAYERS_AMOUNT\n",
    "        layers[0] = learn_data[batch_start:batch_end]\n",
    "        layers[1] = tanh(np.dot(layers[0], weights[0]))\n",
    "        mask = np.random.randint(2, size=layers[1].shape)\n",
    "        layers[1] *= mask\n",
    "        layers[2] = softmax(np.dot(layers[1], weights[1]))\n",
    "\n",
    "        expected_outputs = learn_outputs[batch_start:batch_end]\n",
    "        \n",
    "        for k in range(BATCH_SIZE):\n",
    "            correct_counter += int(np.argmax(layers[2][k]) == np.argmax(expected_outputs[k]))\n",
    "\n",
    "        layer_deltas = [None] * TOTAL_LAYERS_AMOUNT\n",
    "        layer_deltas[2] = (expected_outputs - layers[2]) / (BATCH_SIZE)\n",
    "        layer_deltas[1] = np.dot(layer_deltas[2], weights[1].T) * tanh_deriv(layers[1])\n",
    "        layer_deltas[1] *= mask\n",
    "\n",
    "        weights[1] += np.dot(layers[1].T, layer_deltas[2]) * ALPHA\n",
    "        weights[0] += np.dot(layers[0].T, layer_deltas[1]) * ALPHA\n",
    "    \n",
    "    if (i % 10 == 0):\n",
    "        print(\n",
    "            f\"Iter: {i}\",\n",
    "            f\"Learn-Acc: {correct_counter / LEARN_DATA_SIZE:.2f}\",\n",
    "            end=' '\n",
    "        )\n",
    "\n",
    "        correct_counter = 0\n",
    "\n",
    "        for j in range(len(test_data)):\n",
    "            layers = [None] * TOTAL_LAYERS_AMOUNT\n",
    "            layers[0] = test_data[j].reshape(1, INPUT_SIZE)\n",
    "            layers[1] = tanh(np.dot(layers[0], weights[0]))\n",
    "            layers[2] = np.dot(layers[1], weights[1])\n",
    "\n",
    "            correct_counter += (np.argmax(layers[2]) == np.argmax(test_outputs[j]))\n",
    "        \n",
    "        print(f\"Test-Acc: {correct_counter / len(test_data):.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('nn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b243be397b004ca232554ba897cb822c69cb18e8d26499f4a19cd8366c84d8c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
